{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression: Predicting Continuous Outcomes\n",
    "\n",
    "This notebook demonstrates how to use TESLearn for **continuous outcomes** (e.g., symptom improvement scores, cognitive enhancement) rather than binary classification.\n",
    "\n",
    "## Approach Overview\n",
    "\n",
    "**Feature Extraction**: Atlas-based ROIs  \n",
    "**Feature Selection**: F-regression (for continuous targets)  \n",
    "**Model**: Elastic Net (combines Ridge and Lasso)  \n",
    "**Validation**: K-Fold Cross-Validation\n",
    "\n",
    "## When to Use Regression\n",
    "\n",
    "- **Symptom severity**: Predicting degree of improvement (not just responder vs. non-responder)\n",
    "- **Cognitive enhancement**: Memory, attention, executive function scores\n",
    "- **Dose-response**: Relationship between E-field intensity and clinical outcome\n",
    "- **Personalized dosing**: Predicting optimal stimulation parameters\n",
    "\n",
    "## Why Elastic Net?\n",
    "\n",
    "Elastic Net combines the benefits of Ridge (L2) and Lasso (L1) regularization:\n",
    "- **Feature selection**: L1 component zeros out unimportant features\n",
    "- **Stability**: L2 component handles correlated features better than pure Lasso\n",
    "- **Interpretability**: Sparse solutions with selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teslearn as tl\n",
    "from teslearn.data import load_dataset_from_csv, NiftiLoader\n",
    "from teslearn.features import AtlasFeatureExtractor\n",
    "from teslearn.models import ElasticNetModel\n",
    "from teslearn.selection import FRegressionSelector\n",
    "from teslearn.cv import KFoldValidator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "**Key difference**: Set `task='regression'` to specify continuous targets. The CSV should have a continuous column for the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with continuous targets\n",
    "dataset = load_dataset_from_csv(\n",
    "    csv_path='data/subjects_regression.csv',\n",
    "    target_col='improvement_score',  # Continuous variable\n",
    "    task='regression'\n",
    ")\n",
    "\n",
    "loader = NiftiLoader()\n",
    "images, indices = loader.load_dataset_images(dataset)\n",
    "y = dataset.get_targets()\n",
    "\n",
    "print(f\"Dataset: {len(images)} subjects\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(f\"  Mean: {y.mean():.2f}\")\n",
    "print(f\"  Std: {y.std():.2f}\")\n",
    "print(f\"  Range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis\n",
    "\n",
    "Always visualize your target distribution before modeling. Check for:\n",
    "- Normality (for parametric methods)\n",
    "- Outliers (may need robust methods)\n",
    "- Range (consider standardization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(y, bins=15, edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(y.mean(), color='red', linestyle='--', label=f'Mean: {y.mean():.2f}')\n",
    "axes[0].set_xlabel('Improvement Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Target Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "# Q-Q plot for normality\n",
    "stats.probplot(y, dist=\"norm\", plot=axes[1])\n",
    "axes[1].set_title('Q-Q Plot (Normality Check)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "shapiro_stat, shapiro_p = stats.shapiro(y)\n",
    "print(f\"\\nShapiro-Wilk test: W={shapiro_stat:.4f}, p={shapiro_p:.4f}\")\n",
    "print(f\"Distribution is {'normal' if shapiro_p > 0.05 else 'non-normal'} (α=0.05)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "Same as classification, but we'll use different selection and modeling approaches appropriate for continuous targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract atlas-based features\n",
    "extractor = AtlasFeatureExtractor(\n",
    "    atlas_path='data/atlas/HCP-MMP1.nii.gz',\n",
    "    statistics=['mean', 'std'],  # Standard deviation captures variability\n",
    "    top_percentile=90.0\n",
    ")\n",
    "\n",
    "X = extractor.fit_transform(images)\n",
    "print(f\"Feature matrix: {X.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection for Regression\n",
    "\n",
    "**F-regression** performs univariate linear regression tests and selects features with highest F-scores.\n",
    "\n",
    "**Why not T-test?** T-tests compare group means (binary), while F-regression tests linear relationships (continuous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-regression for continuous targets\n",
    "selector = FRegressionSelector(\n",
    "    p_threshold=0.01,      # Less conservative than classification\n",
    "    correction='fdr'       # False Discovery Rate control\n",
    ")\n",
    "\n",
    "# Test selection\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "print(f\"Selected {X_selected.shape[1]} / {X.shape[1]} features\")\n",
    "print(f\"Reduction: {100 * (1 - X_selected.shape[1]/X.shape[1]):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Elastic Net Configuration\n",
    "\n",
    "**Elastic Net formula**: L1_ratio controls the mix:\n",
    "- `l1_ratio=1.0`: Pure Lasso (sparse solutions)\n",
    "- `l1_ratio=0.0`: Pure Ridge (all features, small coefficients)\n",
    "- `l1_ratio=0.5`: Balanced mix (recommended starting point)\n",
    "\n",
    "**Alpha**: Overall regularization strength (higher = more regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Elastic Net\n",
    "model = ElasticNetModel(\n",
    "    alpha=0.5,             # Regularization strength\n",
    "    l1_ratio=0.5,          # Balance between L1 and L2\n",
    "    max_iter=2000,         # May need more iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Elastic Net Configuration:\")\n",
    "print(f\"  Alpha: {model.alpha}\")\n",
    "print(f\"  L1 ratio: {model.l1_ratio}\")\n",
    "print(f\"  Regularization: {'Lasso-like' if model.l1_ratio > 0.7 else 'Ridge-like' if model.l1_ratio < 0.3 else 'Balanced'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training with Cross-Validation\n",
    "\n",
    "For regression, we use:\n",
    "- **R² score**: Explained variance (0 = mean predictor, 1 = perfect prediction)\n",
    "- **MAE**: Mean Absolute Error (interpretable in original units)\n",
    "- **RMSE**: Root Mean Squared Error (punishes large errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold CV for regression\n",
    "outer_cv = KFoldValidator(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = KFoldValidator(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Train model\n",
    "result = tl.train_model(\n",
    "    images=images,\n",
    "    y=y,\n",
    "    feature_extractor=extractor,\n",
    "    model=model,\n",
    "    feature_selector=selector,\n",
    "    outer_validator=outer_cv,\n",
    "    inner_validator=inner_cv,\n",
    "    use_scaling=True\n",
    ")\n",
    "\n",
    "print(result.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizing Predictions\n",
    "\n",
    "For regression, visualize:\n",
    "1. Predicted vs Actual scatter plot (should follow diagonal)\n",
    "2. Residual distribution (should be centered at 0)\n",
    "3. Feature importance (coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Get predictions from CV\n",
    "if hasattr(result, 'all_y_true') and hasattr(result, 'all_y_pred'):\n",
    "    y_true = result.all_y_true\n",
    "    y_pred = result.all_y_pred\n",
    "    \n",
    "    # 1. Predicted vs Actual\n",
    "    axes[0, 0].scatter(y_true, y_pred, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    min_val, max_val = min(y_true.min(), y_pred.min()), max(y_true.max(), y_pred.max())\n",
    "    axes[0, 0].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect prediction')\n",
    "    axes[0, 0].set_xlabel('Actual Improvement Score')\n",
    "    axes[0, 0].set_ylabel('Predicted Improvement Score')\n",
    "    axes[0, 0].set_title(f'Predicted vs Actual (R² = {result.mean_r2:.3f})')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Add correlation\n",
    "    corr, p_val = stats.pearsonr(y_true, y_pred)\n",
    "    axes[0, 0].text(0.05, 0.95, f'r = {corr:.3f}, p = {p_val:.4f}', \n",
    "                    transform=axes[0, 0].transAxes, verticalalignment='top',\n",
    "                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    # 2. Residuals\n",
    "    residuals = y_true - y_pred\n",
    "    axes[0, 1].scatter(y_pred, residuals, alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Predicted Values')\n",
    "    axes[0, 1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "    axes[0, 1].set_title('Residual Plot')\n",
    "    \n",
    "    # 3. Residual distribution\n",
    "    axes[1, 0].hist(residuals, bins=15, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(x=0, color='red', linestyle='--')\n",
    "    axes[1, 0].set_xlabel('Residual Value')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title(f'Residual Distribution (Mean: {residuals.mean():.3f})')\n",
    "\n",
    "# 4. Feature importance from fitted model\n",
    "result.pipeline.fit(images, y)\n",
    "importance = result.pipeline.get_feature_importance()\n",
    "\n",
    "if importance:\n",
    "    # Sort by absolute importance\n",
    "    sorted_imp = sorted(importance.items(), key=lambda x: abs(x[1]), reverse=True)[:15]\n",
    "    names, values = zip(*sorted_imp)\n",
    "    \n",
    "    colors = ['green' if v > 0 else 'red' for v in values]\n",
    "    axes[1, 1].barh(range(len(values)), values, color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_yticks(range(len(values)))\n",
    "    axes[1, 1].set_yticklabels(names, fontsize=8)\n",
    "    axes[1, 1].set_xlabel('Coefficient Value')\n",
    "    axes[1, 1].set_title('Top 15 Features (Elastic Net Coefficients)')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interpreting Coefficients\n",
    "\n",
    "Elastic Net coefficients tell us:\n",
    "- **Magnitude**: How strongly the ROI predicts outcome\n",
    "- **Sign**: Positive = higher E-field predicts better outcome\n",
    "- **Zero**: Feature was excluded by L1 regularization\n",
    "\n",
    "**Important**: Coefficients are in standardized units (due to scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get non-zero coefficients\n",
    "nonzero = {k: v for k, v in importance.items() if abs(v) > 1e-6}\n",
    "print(f\"\\nModel used {len(nonzero)} / {len(importance)} features\")\n",
    "print(f\"Sparsity: {100 * (1 - len(nonzero)/len(importance)):.1f}%\\n\")\n",
    "\n",
    "print(\"Top 10 predictive regions:\")\n",
    "for i, (region, coef) in enumerate(sorted_imp[:10], 1):\n",
    "    direction = \"↑\" if coef > 0 else \"↓\"\n",
    "    print(f\"{i:2d}. {region}: {coef:+.4f} {direction}\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  ↑ Positive coefficients: Higher E-field in this region → Better outcome\")\n",
    "print(\"  ↓ Negative coefficients: Higher E-field in this region → Worse outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Use F-regression** for continuous targets (not T-test)\n",
    "2. **Elastic Net** provides both prediction and feature selection\n",
    "3. **Check residuals** to validate model assumptions\n",
    "4. **R² interpretation**: Values < 0.1 are common in neuroimaging; focus on cross-validation stability\n",
    "\n",
    "## Comparison: Classification vs Regression\n",
    "\n",
    "| Aspect | Classification | Regression |\n",
    "|--------|---------------|------------|\n",
    "| Target | Binary (0/1) | Continuous |\n",
    "| Selection | T-test | F-regression |\n",
    "| Metric | Accuracy, AUC | R², MAE, RMSE |\n",
    "| Interpretation | Feature importance | Coefficients (direction matters) |\n",
    "| Use case | Responder prediction | Symptom severity, dose optimization |\n",
    "\n",
    "## Best Practices for Regression\n",
    "\n",
    "- Always check target distribution (transform if highly skewed)\n",
    "- Report both R² and MAE/RMSE\n",
    "- Validate residuals are normally distributed\n",
    "- Consider robust regression for outliers\n",
    "- Use L1_ratio tuning if many correlated features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
