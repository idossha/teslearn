{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Grid Search\n",
    "\n",
    "This notebook demonstrates systematic hyperparameter optimization using grid search within nested cross-validation. This is essential for maximizing model performance while avoiding overfitting.\n",
    "\n",
    "## Why Hyperparameter Tuning Matters\n",
    "\n",
    "Default parameters rarely yield optimal performance. Tuning allows you to:\n",
    "- **Adapt to your data**: Small samples need stronger regularization\n",
    "- **Optimize for your metric**: Accuracy, AUC, or sensitivity may require different settings\n",
    "- **Prevent overfitting**: Proper regularization is critical with high-dimensional neuroimaging data\n",
    "\n",
    "## The Challenge: Data Leakage\n",
    "\n",
    "**Problem**: If you optimize hyperparameters on the test set, you overfit the evaluation.\n",
    "\n",
    "**Solution**: Nested cross-validation\n",
    "- **Outer loop**: Evaluate final performance (unbiased)\n",
    "- **Inner loop**: Optimize hyperparameters (can overfit, but that's OK)\n",
    "\n",
    "## Approach Overview\n",
    "\n",
    "**Feature Extraction**: Atlas-based ROIs  \n",
    "**Feature Selection**: T-test with variable thresholds  \n",
    "**Model**: Logistic Regression with tuned C and penalty  \n",
    "**Validation**: Nested CV with GridSearch in inner loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teslearn as tl\n",
    "from teslearn.data import load_dataset_from_csv, NiftiLoader\n",
    "from teslearn.features import AtlasFeatureExtractor\n",
    "from teslearn.models import LogisticRegressionModel\n",
    "from teslearn.selection import TTestSelector\n",
    "from teslearn.cv import StratifiedKFoldValidator\n",
    "from teslearn.pipeline import TESPipeline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "Load your dataset as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset_from_csv(\n",
    "    csv_path='data/subjects.csv',\n",
    "    target_col='response',\n",
    "    task='classification'\n",
    ")\n",
    "\n",
    "loader = NiftiLoader()\n",
    "images, indices = loader.load_dataset_images(dataset)\n",
    "y = dataset.get_targets()\n",
    "\n",
    "print(f\"Dataset: {len(images)} subjects, {sum(y)} responders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Hyperparameter Search Space\n",
    "\n",
    "**Key parameters to tune**:\n",
    "\n",
    "1. **Feature Selection Threshold**: How strict should the statistical test be?\n",
    "   - Too loose: Many features, risk of overfitting\n",
    "   - Too strict: Lose informative features\n",
    "\n",
    "2. **Regularization Strength (C)**: Inverse of regularization\n",
    "   - Small C: Strong regularization (good for small samples)\n",
    "   - Large C: Weak regularization (good for large samples)\n",
    "\n",
    "3. **Penalty Type**: L1 (sparse) vs L2 (dense) vs Elastic Net\n",
    "\n",
    "**Best Practice**: Start with a coarse grid, then refine around promising regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'model__C': [0.01, 0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "    'model__penalty': ['l1', 'l2'],              # Lasso vs Ridge\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter search space:\")\n",
    "print(f\"  C values: {param_grid['model__C']}\")\n",
    "print(f\"  Penalties: {param_grid['model__penalty']}\")\n",
    "print(f\"  Total combinations: {len(param_grid['model__C']) * len(param_grid['model__penalty'])}\")\n",
    "\n",
    "# For feature selection threshold, we'll test a few values\n",
    "p_thresholds = [0.001, 0.01, 0.05]\n",
    "print(f\"\\nP-value thresholds to test: {p_thresholds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "Extract features once - feature extraction doesn't have hyperparameters to tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "extractor = AtlasFeatureExtractor(\n",
    "    atlas_path='data/atlas/HCP-MMP1.nii.gz',\n",
    "    statistics=['mean', 'max', 'top10mean']\n",
    ")\n",
    "\n",
    "X = extractor.fit_transform(images)\n",
    "print(f\"Extracted {X.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Nested Cross-Validation with Grid Search\n",
    "\n",
    "**Structure**:\n",
    "- Outer loop: 5-fold CV for final evaluation\n",
    "- Inner loop: 3-fold CV for hyperparameter tuning\n",
    "\n",
    "**Important**: We fit the selector BEFORE grid search to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "best_params_per_fold = []\n",
    "\n",
    "# Outer CV\n",
    "outer_cv = StratifiedKFoldValidator(n_splits=5, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFoldValidator(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "fold_idx = 0\n",
    "\n",
    "for train_idx, test_idx in outer_cv.split(X, y):\n",
    "    fold_idx += 1\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Fold {fold_idx}/5\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    images_train = [images[i] for i in train_idx]\n",
    "    images_test = [images[i] for i in test_idx]\n",
    "    \n",
    "    # Test different p-value thresholds\n",
    "    best_score = 0\n",
    "    best_pipeline = None\n",
    "    best_params = None\n",
    "    \n",
    "    for p_thresh in p_thresholds:\n",
    "        # Feature selection (fit on training data only)\n",
    "        selector = TTestSelector(p_threshold=p_thresh, correction=None)\n",
    "        selector.fit(X_train, y_train)\n",
    "        X_train_selected = selector.transform(X_train)\n",
    "        \n",
    "        # Skip if too few features selected\n",
    "        if X_train_selected.shape[1] < 5:\n",
    "            print(f\"  p={p_thresh}: Too few features ({X_train_selected.shape[1]}), skipping\")\n",
    "            continue\n",
    "        \n",
    "        # Create pipeline\n",
    "        model = LogisticRegressionModel(solver='lbfgs', max_iter=1000, random_state=42)\n",
    "        pipeline = TESPipeline(\n",
    "            feature_extractor=extractor,\n",
    "            feature_selector=selector,\n",
    "            model=model,\n",
    "            use_scaling=True\n",
    "        )\n",
    "        \n",
    "        # Grid search\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grid,\n",
    "            cv=inner_cv,\n",
    "            scoring='roc_auc',\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Fit grid search\n",
    "        grid_search.fit(images_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        y_pred_proba = grid_search.predict_proba(images_test)[:, 1]\n",
    "        test_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        print(f\"  p={p_thresh}: Best params: {grid_search.best_params_}, Test AUC: {test_auc:.3f}\")\n",
    "        \n",
    "        if test_auc > best_score:\n",
    "            best_score = test_auc\n",
    "            best_pipeline = grid_search.best_estimator_\n",
    "            best_params = {\n",
    "                'p_threshold': p_thresh,\n",
    "                **grid_search.best_params_\n",
    "            }\n",
    "    \n",
    "    # Store results for this fold\n",
    "    y_pred = best_pipeline.predict(images_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    results.append({\n",
    "        'fold': fold_idx,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': best_score,\n",
    "        'n_features': X_train_selected.shape[1] if 'X_train_selected' in locals() else 0,\n",
    "    })\n",
    "    best_params_per_fold.append(best_params)\n",
    "    \n",
    "    print(f\"\\nFold {fold_idx} Best: {best_params}\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}, AUC: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results\n",
    "\n",
    "Look at:\n",
    "1. Performance stability across folds\n",
    "2. Most frequently selected hyperparameters\n",
    "3. Relationship between regularization and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"Cross-Validation Results Summary:\")\n",
    "print(f\"  Mean Accuracy: {results_df['accuracy'].mean():.3f} (+/- {results_df['accuracy'].std():.3f})\")\n",
    "print(f\"  Mean AUC: {results_df['auc'].mean():.3f} (+/- {results_df['auc'].std():.3f})\")\n",
    "print(f\"\\nPer-fold performance:\")\n",
    "print(results_df)\n",
    "\n",
    "# Analyze selected hyperparameters\n",
    "print(\"\\nHyperparameter Selection Frequency:\")\n",
    "for param_name in ['model__C', 'model__penalty', 'p_threshold']:\n",
    "    values = [p[param_name] for p in best_params_per_fold]\n",
    "    print(f\"\\n{param_name}:\")\n",
    "    for val in set(values):\n",
    "        count = values.count(val)\n",
    "        print(f\"  {val}: {count}/5 folds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Hyperparameter Landscape\n",
    "\n",
    "Understanding how different hyperparameters affect performance helps build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Performance across folds\n",
    "axes[0, 0].bar(results_df['fold'], results_df['auc'], color='steelblue', alpha=0.7)\n",
    "axes[0, 0].axhline(results_df['auc'].mean(), color='red', linestyle='--', \n",
    "                   label=f\"Mean: {results_df['auc'].mean():.3f}\")\n",
    "axes[0, 0].set_xlabel('Fold')\n",
    "axes[0, 0].set_ylabel('AUC')\n",
    "axes[0, 0].set_title('Performance Across Folds')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# 2. C parameter distribution\n",
    "c_values = [p['model__C'] for p in best_params_per_fold]\n",
    "c_counts = pd.Series(c_values).value_counts().sort_index()\n",
    "axes[0, 1].bar(range(len(c_counts)), c_counts.values, color='coral')\n",
    "axes[0, 1].set_xticks(range(len(c_counts)))\n",
    "axes[0, 1].set_xticklabels(c_counts.index)\n",
    "axes[0, 1].set_xlabel('C Parameter')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Selected C Values')\n",
    "\n",
    "# 3. Penalty type distribution\n",
    "penalty_values = [p['model__penalty'] for p in best_params_per_fold]\n",
    "penalty_counts = pd.Series(penalty_values).value_counts()\n",
    "axes[1, 0].pie(penalty_counts.values, labels=penalty_counts.index, autopct='%1.0f%%',\n",
    "               colors=['lightblue', 'lightcoral'])\n",
    "axes[1, 0].set_title('Penalty Type Distribution')\n",
    "\n",
    "# 4. P-threshold distribution\n",
    "p_values = [p['p_threshold'] for p in best_params_per_fold]\n",
    "p_counts = pd.Series(p_values).value_counts().sort_index()\n",
    "axes[1, 1].bar(range(len(p_counts)), p_counts.values, color='lightgreen')\n",
    "axes[1, 1].set_xticks(range(len(p_counts)))\n",
    "axes[1, 1].set_xticklabels([f\"{p:.3f}\" for p in p_counts.index])\n",
    "axes[1, 1].set_xlabel('P-value Threshold')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Selected P-value Thresholds')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Model Training\n",
    "\n",
    "After hyperparameter tuning, train the final model on all data using the most common hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Select most common hyperparameters\n",
    "final_c = Counter([p['model__C'] for p in best_params_per_fold]).most_common(1)[0][0]\n",
    "final_penalty = Counter([p['model__penalty'] for p in best_params_per_fold]).most_common(1)[0][0]\n",
    "final_p = Counter([p['p_threshold'] for p in best_params_per_fold]).most_common(1)[0][0]\n",
    "\n",
    "print(\"Final Model Hyperparameters (majority vote):\")\n",
    "print(f\"  C: {final_c}\")\n",
    "print(f\"  Penalty: {final_penalty}\")\n",
    "print(f\"  P-threshold: {final_p}\")\n",
    "\n",
    "# Train final pipeline\n",
    "final_selector = TTestSelector(p_threshold=final_p, correction=None)\n",
    "final_model = LogisticRegressionModel(\n",
    "    C=final_c,\n",
    "    penalty=final_penalty,\n",
    "    solver='lbfgs' if final_penalty == 'l2' else 'liblinear',\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_pipeline = TESPipeline(\n",
    "    feature_extractor=extractor,\n",
    "    feature_selector=final_selector,\n",
    "    model=final_model,\n",
    "    use_scaling=True\n",
    ")\n",
    "\n",
    "# Fit on all data\n",
    "final_pipeline.fit(images, y)\n",
    "\n",
    "print(f\"\\nFinal model trained on {len(images)} subjects\")\n",
    "print(f\"Selected {len(final_pipeline.feature_importance_)} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Always use nested CV**: Prevents hyperparameter overfitting\n",
    "2. **Grid search is expensive**: Consider RandomizedSearchCV for larger spaces\n",
    "3. **Cross-validation stability**: Look at variance across folds, not just mean\n",
    "4. **Majority vote**: Most common hyperparameters often work well\n",
    "\n",
    "## Hyperparameter Tuning Checklist\n",
    "\n",
    "- [ ] Define parameter grid (coarse â†’ fine)\n",
    "- [ ] Use appropriate inner CV (3-5 fold)\n",
    "- [ ] Select scoring metric aligned with goal (AUC for imbalanced, accuracy for balanced)\n",
    "- [ ] Analyze stability across outer folds\n",
    "- [ ] Consider computational cost (fewer folds if needed)\n",
    "- [ ] Validate on held-out test set (if available)\n",
    "\n",
    "## Advanced Techniques\n",
    "\n",
    "1. **Bayesian Optimization**: More efficient than grid search (use `scikit-optimize`)\n",
    "2. **Randomized Search**: Sample from distributions instead of grid\n",
    "3. **Early Stopping**: Stop if performance plateaus\n",
    "4. **Nested permutation tests**: Validate that tuned model beats chance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
