{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel-Based Analysis with SVM\n",
    "\n",
    "This notebook demonstrates a voxel-based approach using Support Vector Machines (SVM). This is useful when you want fine-grained spatial resolution beyond atlas parcellations.\n",
    "\n",
    "## Approach Overview\n",
    "\n",
    "**Feature Extraction**: Voxel-based (statistically selected)  \n",
    "**Feature Selection**: VoxelSelectorFromImages (univariate selection)  \n",
    "**Model**: Support Vector Machine with RBF kernel  \n",
    "**Validation**: Leave-One-Out Cross-Validation\n",
    "\n",
    "## When to Use This\n",
    "\n",
    "- **Small, focal effects**: When TES effects are localized to specific voxels rather than entire regions\n",
    "- **Exploratory analysis**: To identify optimal stimulation sites without atlas constraints\n",
    "- **High spatial precision**: When millimeter-level accuracy matters\n",
    "\n",
    "## Cautions\n",
    "\n",
    "- **Curse of dimensionality**: Voxel features = ~50,000-200,000 dimensions\n",
    "- **Overfitting risk**: Must use aggressive feature selection\n",
    "- **Computational cost**: SVM training is O(n²) to O(n³)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import teslearn as tl\n",
    "from teslearn.data import load_dataset_from_csv, NiftiLoader\n",
    "from teslearn.features import VoxelFeatureExtractor\n",
    "from teslearn.models import SVMModel\n",
    "from teslearn.selection import VoxelSelectorFromImages\n",
    "from teslearn.cv import LeaveOneOutValidator, StratifiedKFoldValidator\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "**Best Practice**: For voxel-based analysis, ensure all images are in the same space (MNI) and have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset_from_csv(\n",
    "    csv_path='data/subjects.csv',\n",
    "    target_col='response',\n",
    "    task='classification'\n",
    ")\n",
    "\n",
    "loader = NiftiLoader()\n",
    "images, indices = loader.load_dataset_images(dataset)\n",
    "y = dataset.get_targets()\n",
    "\n",
    "print(f\"Dataset: {len(images)} subjects\")\n",
    "print(f\"Image shape: {images[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Voxel Selection Strategy\n",
    "\n",
    "**Critical Step**: We cannot use all voxels (curse of dimensionality). Instead:\n",
    "\n",
    "1. **Univariate selection**: Select voxels showing group differences\n",
    "2. **Spatial clustering**: Keep only significant clusters (optional)\n",
    "3. **Conservative threshold**: p < 0.001 or stricter\n",
    "\n",
    "**Why this matters**: Including non-informative voxels adds noise and increases overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select voxels based on univariate statistical test\n",
    "voxel_selector = VoxelSelectorFromImages(\n",
    "    p_threshold=0.001,      # Conservative p-value\n",
    "    test_type='ttest',      # T-test for binary classification\n",
    "    correction=None         # Can use 'bonferroni' for strict correction\n",
    ")\n",
    "\n",
    "# Fit selector on training data\n",
    "voxel_selector.fit(images, y)\n",
    "\n",
    "# Get selected coordinates\n",
    "selected_coords = voxel_selector.get_voxel_coordinates()\n",
    "print(f\"Selected {len(selected_coords)} voxels from {np.prod(images[0].shape)} total\")\n",
    "print(f\"Reduction: {100 * (1 - len(selected_coords)/np.prod(images[0].shape)):.1f}%\")\n",
    "\n",
    "# Create extractor for selected voxels\n",
    "extractor = voxel_selector.create_voxel_extractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Selected Voxels\n",
    "\n",
    "Visualize the spatial distribution of selected voxels to understand where predictive information lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask of selected voxels\n",
    "mask = np.zeros(images[0].shape, dtype=bool)\n",
    "for x, y_coord, z in selected_coords:\n",
    "    mask[x, y_coord, z] = True\n",
    "\n",
    "# Plot distribution across slices\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "sample_img = images[0]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    z_slice = 20 + i * 10\n",
    "    if z_slice < sample_img.shape[2]:\n",
    "        ax.imshow(sample_img[:, :, z_slice], cmap='gray', alpha=0.5)\n",
    "        ax.imshow(mask[:, :, z_slice], cmap='Reds', alpha=0.7)\n",
    "        ax.set_title(f'Z={z_slice}')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Selected Voxels (Red) Overlay on Sample Image')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. SVM Configuration\n",
    "\n",
    "**Support Vector Machines** are well-suited for high-dimensional data:\n",
    "\n",
    "- **Kernel trick**: RBF kernel handles non-linear relationships\n",
    "- **Margin maximization**: Naturally resistant to overfitting\n",
    "- **Works in high-D**: Effective even when features > samples\n",
    "\n",
    "**Hyperparameters**:\n",
    "- `C`: Regularization (smaller = stronger regularization)\n",
    "- `gamma`: Kernel coefficient ('scale' is usually good)\n",
    "- `probability=True`: Enable probability estimates (needed for ROC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SVM\n",
    "model = SVMModel(\n",
    "    kernel='rbf',           # Radial basis function kernel\n",
    "    C=1.0,                  # Regularization parameter\n",
    "    gamma='scale',          # Kernel coefficient\n",
    "    class_weight='balanced', # Handle class imbalance\n",
    "    probability=True,       # Enable probability estimates\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"SVM Configuration:\")\n",
    "print(f\"  Kernel: RBF\")\n",
    "print(f\"  C: {model.C}\")\n",
    "print(f\"  Gamma: {model.gamma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leave-One-Out Cross-Validation\n",
    "\n",
    "**When to use LOO**:\n",
    "- Very small sample sizes (n < 30)\n",
    "- When every subject is precious\n",
    "- Unbiased but high variance estimates\n",
    "\n",
    "**Trade-off**: LOO gives nearly unbiased estimates but has higher variance than k-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure cross-validation\n",
    "outer_cv = LeaveOneOutValidator()  # One subject left out for testing\n",
    "inner_cv = StratifiedKFoldValidator(n_splits=3)  # 3-fold for feature selection\n",
    "\n",
    "# Build pipeline\n",
    "from teslearn.pipeline import TESPipeline\n",
    "\n",
    "pipeline = TESPipeline(\n",
    "    feature_extractor=extractor,\n",
    "    feature_selector=None,  # Already selected voxels\n",
    "    model=model,\n",
    "    use_scaling=True        # Critical for SVM\n",
    ")\n",
    "\n",
    "# Perform cross-validation\n",
    "result = tl.cross_validate(\n",
    "    pipeline=pipeline,\n",
    "    images=images,\n",
    "    y=y,\n",
    "    outer_validator=outer_cv,\n",
    "    inner_validator=inner_cv\n",
    ")\n",
    "\n",
    "print(result.get_summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Interpret Voxel Importance\n",
    "\n",
    "SVMs don't provide direct feature importance like logistic regression. However, we can:\n",
    "1. Use permutation importance\n",
    "2. Analyze support vectors\n",
    "3. Map decision boundary in voxel space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from teslearn.viz import create_stat_map\n",
    "\n",
    "# Extract features for visualization\n",
    "X = extractor.fit_transform(images)\n",
    "\n",
    "# Fit model on all data for interpretation\n",
    "pipeline.fit(images, y)\n",
    "\n",
    "# Get predictions and decision values\n",
    "decision_values = pipeline.model._model.decision_function(X)\n",
    "\n",
    "print(f\"Decision value range: [{decision_values.min():.2f}, {decision_values.max():.2f}]\")\n",
    "print(f\"Mean absolute decision value: {np.abs(decision_values).mean():.2f}\")\n",
    "\n",
    "# Analyze class separation\n",
    "responders = decision_values[y == 1]\n",
    "non_responders = decision_values[y == 0]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(non_responders, bins=15, alpha=0.7, label='Non-responders', color='red')\n",
    "plt.hist(responders, bins=15, alpha=0.7, label='Responders', color='blue')\n",
    "plt.axvline(x=0, color='black', linestyle='--', label='Decision boundary')\n",
    "plt.xlabel('SVM Decision Value')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Separation in Decision Space')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Voxel selection is critical**: Never use raw voxels; always select based on univariate statistics\n",
    "2. **SVM works well in high-D**: RBF kernel handles non-linear relationships in voxel space\n",
    "3. **Scaling is essential**: Always standardize features for SVM\n",
    "4. **LOO for small N**: Use when sample size is very limited\n",
    "\n",
    "## Comparison to Atlas Approach\n",
    "\n",
    "| Aspect | Atlas-Based | Voxel-Based |\n",
    "|--------|-------------|-------------|\n",
    "| Resolution | Regional | Sub-regional |\n",
    "| Interpretability | High (anatomical) | Lower (coordinate space) |\n",
    "| Overfitting risk | Low | Higher |\n",
    "| Sample size | Flexible | Requires larger N |\n",
    "| Computational cost | Low | Higher |\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "- Always validate voxel selection within CV loops\n",
    "- Use conservative p-thresholds (0.001 or Bonferroni)\n",
    "- Consider spatial smoothing before voxel selection\n",
    "- Report number of selected voxels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
